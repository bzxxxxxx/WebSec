# Crawlers

What do we mean by crawlers when we speak in context with the world wide web?

Crawlers are nothing but applications that visit all the sites and note what it contains. All major search companies use a crawler. That's how they find the websites when we give a piece of text from it. Each crawler identifies itself using the User-Agent request header.

The major use of robot.txt is to restrict search engine crawlers from indexing selected area of your website, so in simple words it basically means that robot.txt doesnt allow web crawlers to have complete access to the website.
